{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MIAD modelado de datos y ETL - Primer ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.types import FloatType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style()\n",
    "pd.set_option(\"display.max_columns\", None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'D:\\\\spark\\\\spark-3.1.2-bin-hadoop2.7'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[2] pyspark-shell'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Configuración de la sesión\n",
    "\n",
    "appName = \"PySpark SQL/MYSQL - via JDBC\"\n",
    "master = \"local\"\n",
    "conf = SparkConf()\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\\\n",
    "    .set(\"spark.driver.extraClassPath\",\"C:\\dev\\sqljdbc_9.4\\enu\\mssql-jdbc-9.4.0.jre8.jar;C:\\Program Files (x86)\\MySQL\\Connector J 8.0\\mysql-connector-java-8.0.26.jar\")\n",
    "spark_context = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(spark_context)\n",
    "spark = sql_context.sparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x190a714da90>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://LAPTOP-RRMH1QSM:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySpark SQL/MYSQL - via JDBC</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Configuración servidor base de datos multidimensional\n",
    "user_md = 'user'\n",
    "psswd_md = 'rw,.12a'\n",
    "db_multidimensional_connection_string = \"jdbc:sqlserver://LAPTOP-RRMH1QSM:1433;databaseName=infra_visible_v2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ETL\n",
    "Funciones para la extracción, transformación y guardado de datos."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "# Extraer data from CSV\n",
    "PATH = \"./data/\"\n",
    "\n",
    "def extract_aeropuertos(spark):\n",
    "    return spark.read.load(PATH + \"aeropuertos_cambios_infraestructura.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "def extract_vuelos(spark):\n",
    "    return spark.read.load(PATH + \"vuelosEtapa2.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "def extract_otros(spark):\n",
    "    df = spark.read.load(PATH + \"vuelosEtapa2.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "    empresa_df = df.select(\"empresa\").distinct()\n",
    "    equipo_df = df.select(\"tipo_equipo\").distinct().withColumnRenamed(\"tipo_equipo\", \"equipo\")\n",
    "    tipo_vuelotrafico_df = df.select('trafico','tipo_vuelo').distinct().withColumnRenamed(\"tipo_vuelo\", \"vuelo\")\n",
    "    return empresa_df, equipo_df, tipo_vuelotrafico_df\n",
    "\n",
    "def extract_cobertura(spark):\n",
    "    df_centros = spark.read.load(PATH + \"cobertura_centro.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "    for col in df_centros.columns:\n",
    "        df_centros = df_centros.withColumnRenamed(col,col.replace(\" \", \"_\"))\n",
    "\n",
    "    return df_centros\n",
    "\n",
    "def extract_categoria(df_centros):\n",
    "    dim_categoria_aeropuerto= df_centros.selectExpr('substr(Tipo_Cobertura,1,1) as id_categoria','Tipo_Cobertura as categoria').distinct().sort('Tipo_Cobertura')\n",
    "    return dim_categoria_aeropuerto.withColumnRenamed(\"categoria\", \"nombrecategoria\")\n",
    "\n",
    "def extract_centro_poblado(spark, df_centros):\n",
    "    df_geocentro = spark.read.load(PATH + \"codigos_centro_poblados.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "    df_geocentro = df_geocentro.withColumnRenamed(\"Código Centro Poblado\", 'id_centro_poblado')\n",
    "    df_geocentro = df_geocentro.withColumnRenamed(\"Nombre Municipio\", 'municipio')\n",
    "    df_geocentro = df_geocentro.withColumnRenamed(\"Nombre Departamento\", 'departamento')\n",
    "    df_geocentro = df_geocentro.select('id_centro_poblado','municipio','departamento')\n",
    "\n",
    "    dim_centro_poblado = df_centros.selectExpr('Codigo_Centro_Poblado as id_centro_poblado','nombre_centro_poblado as nombre_centro_poblado')\n",
    "    dim_centro_poblado = dim_centro_poblado.join(df_geocentro, how = 'inner', on='id_centro_poblado')\n",
    "    return dim_centro_poblado\n",
    "\n",
    "# SQL server\n",
    "# Necesito leer las dimensiones fecha y aeropuertos para extraer sus llaves primarias y asignarlas a la tabla de hechos.\n",
    "def extract_aeropuertos_from_multidimensional(spark):\n",
    "    return  spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", db_multidimensional_connection_string)\\\n",
    "        .option(\"dbtable\", \"aeropuertos\")\\\n",
    "        .option(\"user\", user_md)\\\n",
    "        .option(\"password\", psswd_md)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load().select(\"id\", \"sigla\", \"fecha_inicio\", \"fecha_fin\", \"vigente\")\n",
    "\n",
    "def extract_fecha_from_multidimensional(spark):\n",
    "    df = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", db_multidimensional_connection_string)\\\n",
    "        .option(\"dbtable\", \"fecha\")\\\n",
    "        .option(\"user\", user_md)\\\n",
    "        .option(\"password\", psswd_md)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load()\n",
    "    return df.select(\"id\", \"year\", \"month\", \"day\")\n",
    "\n",
    "\n",
    "def extract_empresa_from_multidimensional(spark):\n",
    "    df = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", db_multidimensional_connection_string)\\\n",
    "        .option(\"dbtable\", \"empresa\")\\\n",
    "        .option(\"user\", user_md)\\\n",
    "        .option(\"password\", psswd_md)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "def extract_equipo_from_multidimensional(spark):\n",
    "    df = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", db_multidimensional_connection_string)\\\n",
    "        .option(\"dbtable\", \"tipo_equipo\")\\\n",
    "        .option(\"user\", user_md)\\\n",
    "        .option(\"password\", psswd_md)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "def extract_tipo_from_multidimensional(spark):\n",
    "    df = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", db_multidimensional_connection_string)\\\n",
    "        .option(\"dbtable\", \"tipo_vuelo_trafico\")\\\n",
    "        .option(\"user\", user_md)\\\n",
    "        .option(\"password\", psswd_md)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load()\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "\n",
    "def transform_cobertura(df_centros, df_aeropuerto, fecha_df, centro_poblado_df):\n",
    "    df1=df_centros.selectExpr('Codigo_Centro_Poblado as id_centro_poblado','Aerodromo as sigla', '\"A\" as id_categoria', 'substr(Tipo_Cobertura,1,1) as id_tipo_cobertura','D_Aerodromo as distancia')\n",
    "    df2=df_centros.selectExpr('Codigo_Centro_Poblado as id_centro_poblado','Regional as sigla', '\"R\" as id_categoria', 'substr(Tipo_Cobertura,1,1) as id_tipo_cobertura','D_Regional as distancia')\n",
    "    df3=df_centros.selectExpr('Codigo_Centro_Poblado as id_centro_poblado','Nacional as sigla', '\"N\" as id_categoria', 'substr(Tipo_Cobertura,1,1) as id_tipo_cobertura','D_Nacional as distancia')\n",
    "    df4=df_centros.selectExpr('Codigo_Centro_Poblado as id_centro_poblado','Internacional as sigla', '\"I\" as id_categoria', 'substr(Tipo_Cobertura,1,1) as id_tipo_cobertura','D_Internacional as distancia')\n",
    "\n",
    "    final_df=df1.union(df2)\\\n",
    "        .union(df3)\\\n",
    "        .union(df4)\n",
    "\n",
    "    df_centros2 = final_df.sort('id_centro_poblado')\n",
    "\n",
    "    df_idaerop = df_aeropuerto.selectExpr('id as id_aeropuerto','sigla').where('vigente=\"S\"')\n",
    "\n",
    "    hechos_cobertura_centros_poblados = df_centros2.join(df_idaerop, how = 'inner', on='sigla')\n",
    "\n",
    "    id_fecha = fecha_df.filter((fecha_df['year'] == 2021) & (fecha_df['month'] == 11) & (fecha_df['day'] == 15)).select('id').first()[0]\n",
    "\n",
    "    hechos_cobertura_centros_poblados = hechos_cobertura_centros_poblados.join(centro_poblado_df, how = 'inner', on='id_centro_poblado')\n",
    "\n",
    "    hechos_cobertura_centros_poblados = hechos_cobertura_centros_poblados.selectExpr('{} as id_fecha'.format(id_fecha), 'id_centro_poblado','id_aeropuerto','id_categoria','id_tipo_cobertura','distancia').sort('id_centro_poblado')\n",
    "\n",
    "    return hechos_cobertura_centros_poblados\n",
    "\n",
    "\n",
    "def transform_aeropuertos(df_aeropuerto, df_categoria):\n",
    "    dim_aeropuerto = df_aeropuerto.filter(df_aeropuerto.nombre != 'CAMILO DAZA No.2')\n",
    "    modificaciones = dim_aeropuerto.groupBy('sigla').count()\n",
    "\n",
    "    dim_aeropuerto = dim_aeropuerto.join(modificaciones, how='inner', on=['sigla'])\n",
    "    dim_aeropuerto = dim_aeropuerto.withColumn('fecha_inicio',\n",
    "                                               F.when((F.col('Ano') == 2014), '2009-01-01')\n",
    "                                               .when((F.col('Ano') == 2015) & (F.col('count') == 2), '2009-01-01')\n",
    "                                               .when((F.col('Ano') == 2015) & (F.col('count') == 3), '2015-01-01')\n",
    "                                               .when((F.col('Ano') == 2016), '2016-01-01')\n",
    "                                               .otherwise(0)\n",
    "                                               )\n",
    "    dim_aeropuerto = dim_aeropuerto.withColumn('fecha_fin',\n",
    "                                               F.when((F.col('Ano') == 2014), '2014-12-31')\n",
    "                                               .when((F.col('Ano') == 2015), '2015-12-31')\n",
    "                                               .when((F.col('Ano') == 2016), '2100-12-31')\n",
    "                                               .otherwise(0)\n",
    "                                               )\n",
    "\n",
    "    dim_aeropuerto = dim_aeropuerto.withColumn('vigente',\n",
    "                                               F.when((F.col('Ano') == 2014), 'N')\n",
    "                                               .when((F.col('Ano') == 2015), 'N')\n",
    "                                               .when((F.col('Ano') == 2016), 'S')\n",
    "                                               .otherwise(0)\n",
    "                                               )\n",
    "\n",
    "\n",
    "    tmp_df = df_categoria.withColumnRenamed(\"id_categoria\", \"id_categoriaaeropuerto\")\n",
    "    dim_aeropuerto = dim_aeropuerto.join(tmp_df, dim_aeropuerto['categoria'] == tmp_df['nombrecategoria'])\n",
    "\n",
    "    dim_aeropuerto = dim_aeropuerto.withColumn(\"iata\", F.col(\"sigla\")).withColumnRenamed(\"Ano\", \"year\").drop(\"count\")\n",
    "\n",
    "    return dim_aeropuerto.select('sigla', 'iata', 'nombre', 'municipio', 'departamento', 'id_categoriaaeropuerto', 'latitud',\n",
    "                                 'longitud', 'propietario', 'explotador', 'longitud_pista', 'ancho_pista', 'pbmo', 'elevacion', 'resolucion', 'fecha_construccion', 'fecha_vigencia', 'clase', 'tipo', 'numero_vuelos_origen', 'gcd_departamento', 'gcd_municipio', 'year', 'fecha_inicio', 'fecha_fin', 'vigente')\n",
    "\n",
    "\n",
    "def transform_vuelos(vuelos_df, aeropuertos_df, fecha_df, empresa_df, equipo_df, tipo_df):\n",
    "    df = vuelos_df.selectExpr('ano', 'mes', 'origen', 'destino', 'sillas as puestos_disponibles', 'carga_ofrecida as carga_ofrecida', 'pasajeros as pasajeros_transportado', 'carga_bordo as carga_transportado', 'vuelos', 'empresa', 'tipo_equipo', 'tipo_vuelo', 'trafico')\n",
    "\n",
    "    df = df.withColumn('date', F.concat(F.col('ano'),F.lit('-'), F.col('mes'), F.lit('-01')))\n",
    "    df = df.withColumn('date', df['date'].cast(DateType()))\n",
    "\n",
    "    df = df.withColumn(\"pasajeros_desaprovechado\", F.col(\"puestos_disponibles\") - F.col(\"pasajeros_transportado\"))\n",
    "    df = df.withColumn(\"carga_desaprovechado\", F.col(\"carga_ofrecida\") - F.col(\"carga_transportado\"))\n",
    "\n",
    "    tmp_df = fecha_df.withColumnRenamed(\"id\", \"id_fecha\")\n",
    "    df = df.join(tmp_df, (df['ano'] == tmp_df['year']) & (df['mes']==tmp_df['month']) & (1==tmp_df['day']))\n",
    "\n",
    "    tmp_df = aeropuertos_df.withColumnRenamed(\"id\", \"id_origen\").withColumnRenamed(\"sigla\", \"sigla_origen\").select(\"id_origen\", \"sigla_origen\", \"fecha_inicio\", \"fecha_fin\")\n",
    "    tmp_df = tmp_df.withColumn('fecha_inicio', tmp_df['fecha_inicio'].cast(DateType()))\n",
    "    tmp_df = tmp_df.withColumn('fecha_fin', tmp_df['fecha_fin'].cast(DateType()))\n",
    "    df = df.join(tmp_df, (df['origen'] == tmp_df['sigla_origen']) & (tmp_df['fecha_inicio'] <= df['date']) & (df['date'] <= tmp_df['fecha_fin']))\n",
    "\n",
    "\n",
    "    tmp_df = aeropuertos_df.withColumnRenamed(\"id\", \"id_destino\").withColumnRenamed(\"sigla\", \"sigla_destino\").select(\"id_destino\", \"sigla_destino\", \"fecha_inicio\", \"fecha_fin\")\n",
    "    tmp_df = tmp_df.withColumn('fecha_inicio', tmp_df['fecha_inicio'].cast(DateType()))\n",
    "    tmp_df = tmp_df.withColumn('fecha_fin', tmp_df['fecha_fin'].cast(DateType()))\n",
    "    df = df.join(tmp_df, (df['destino'] == tmp_df['sigla_destino']) & (tmp_df['fecha_inicio'] <= df['date']) & (df['date'] <= tmp_df['fecha_fin']))\n",
    "\n",
    "    tmp_df = empresa_df.withColumnRenamed(\"id\", \"id_empresa\")\n",
    "    df = df.join(tmp_df, df['empresa'] == tmp_df['empresa'])\n",
    "\n",
    "    tmp_df = equipo_df.withColumnRenamed(\"id\", \"id_equipo\")\n",
    "    df = df.join(tmp_df, df['tipo_equipo'] == tmp_df['equipo'])\n",
    "\n",
    "    tmp_df = tipo_df.withColumnRenamed(\"id\", \"id_tipo\")\n",
    "    df = df.join(tmp_df, (df['tipo_vuelo'] == tmp_df['vuelo']) & (df['trafico'] == tmp_df['trafico']))\n",
    "\n",
    "    return df.select('id_fecha', 'id_origen', 'id_destino','id_equipo', 'id_empresa', 'id_tipo', 'puestos_disponibles','carga_ofrecida','pasajeros_transportado', 'carga_transportado', 'pasajeros_desaprovechado', 'carga_desaprovechado', 'vuelos')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejecución del proceso ETL."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "# ETL categoria_aeropuerto y centro_poblado\n",
    "centro_cobertura_df = extract_cobertura(spark)\n",
    "categoria_aeropuerto_df = extract_categoria(centro_cobertura_df)\n",
    "centro_poblado_df = extract_centro_poblado(spark, centro_cobertura_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#categoria\n",
    "categoria_aeropuerto_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"categoria_aeropuerto\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "#centro_poblado\n",
    "centro_poblado_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"centro_poblado\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# ETL aeropuertos\n",
    "aeropuertos_df = extract_aeropuertos(spark)\n",
    "aeropuertos_df = transform_aeropuertos(aeropuertos_df, categoria_aeropuerto_df)\n",
    "\n",
    "aeropuertos_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"aeropuertos\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "# ETL hecho_cobertura_centro_poblado\n",
    "aeropuertos_df = extract_aeropuertos_from_multidimensional(spark)\n",
    "fecha_df = extract_fecha_from_multidimensional(spark)\n",
    "\n",
    "hechos_cobertura_centros_poblado_df = transform_cobertura(centro_cobertura_df, aeropuertos_df, fecha_df, centro_poblado_df)\n",
    "\n",
    "hechos_cobertura_centros_poblado_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"hecho_cobertura_centro_poblado\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "#ETL empresa, tipo_equipo, tipo_vuelo_trafico\n",
    "empresa_df, equipo_df, tipo_vuelotrafico_df = extract_otros(spark)\n",
    "\n",
    "empresa_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"empresa\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()\n",
    "\n",
    "equipo_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"tipo_equipo\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()\n",
    "\n",
    "\n",
    "tipo_vuelotrafico_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"tipo_vuelo_trafico\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# ETL tabla hechos\n",
    "vuelos_df = extract_vuelos(spark)\n",
    "fecha_df = extract_fecha_from_multidimensional(spark)\n",
    "aeropuertos_df = extract_aeropuertos_from_multidimensional(spark)\n",
    "empresa_df = extract_empresa_from_multidimensional(spark)\n",
    "equipo_df = extract_equipo_from_multidimensional(spark)\n",
    "tipo_vuelotrafico_df = extract_tipo_from_multidimensional(spark)\n",
    "vuelos_df = transform_vuelos(vuelos_df, aeropuertos_df, fecha_df, empresa_df, equipo_df, tipo_vuelotrafico_df)\n",
    "\n",
    "\n",
    "vuelos_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"hecho_vuelos\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}