{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MIAD modelado de datos y ETL - Primer ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.types import FloatType, DoubleType\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style()\n",
    "pd.set_option(\"display.max_columns\", None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'D:\\\\spark\\\\spark-3.1.2-bin-hadoop2.7'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[2] pyspark-shell'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Configuración de la sesión\n",
    "\n",
    "appName = \"PySpark SQL/MYSQL - via JDBC\"\n",
    "master = \"local\"\n",
    "conf = SparkConf()\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\\\n",
    "    .set(\"spark.driver.extraClassPath\",\"C:\\dev\\sqljdbc_9.4\\enu\\mssql-jdbc-9.4.0.jre8.jar;C:\\Program Files (x86)\\MySQL\\Connector J 8.0\\mysql-connector-java-8.0.26.jar\")\n",
    "spark_context = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(spark_context)\n",
    "spark = sql_context.sparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x18a36bee370>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://LAPTOP-RRMH1QSM:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySpark SQL/MYSQL - via JDBC</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Configuración servidor base de datos multidimensional\n",
    "user_md = 'user'\n",
    "psswd_md = 'rw,.12a'\n",
    "db_multidimensional_connection_string = \"jdbc:sqlserver://LAPTOP-RRMH1QSM:1433;databaseName=infra_visible_2\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ETL\n",
    "Funciones para la extracción, transformación y guardado de datos."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# Extraer data from CSV\n",
    "PATH = \"./data/\"\n",
    "\n",
    "def extract_aeropuertos(spark):\n",
    "    return spark.read.load(PATH + \"aeropuertos_cambios_infraestructura.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "def extract_vuelos(spark):\n",
    "    return spark.read.load(PATH + \"vuelosEtapa2.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "def extract_otros(spark):\n",
    "    df = spark.read.load(PATH + \"vuelosEtapa2.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "    empresa_df = df.select(\"empresa\").distinct()\n",
    "    equipo_df = df.select(\"tipo_equipo\").distinct().withColumnRenamed(\"tipo_equipo\", \"equipo\")\n",
    "    tipo_vuelotrafico_df = df.select('trafico','tipo_vuelo').distinct().withColumnRenamed(\"tipo_vuelo\", \"vuelo\")\n",
    "    return empresa_df, equipo_df, tipo_vuelotrafico_df\n",
    "\n",
    "def extract_cobertura(spark):\n",
    "    return spark.read.load(PATH + \"cobertura_centro.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "\n",
    "# SQL server\n",
    "# Necesito leer las dimensiones fecha y aeropuertos para extraer sus llaves primarias y asignarlas a la tabla de hechos.\n",
    "def extract_aeropuertos_from_multidimensional(spark):\n",
    "    return  spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", db_multidimensional_connection_string)\\\n",
    "        .option(\"dbtable\", \"aeropuertos\")\\\n",
    "        .option(\"user\", user_md)\\\n",
    "        .option(\"password\", psswd_md)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load().select(\"id\", \"sigla\", \"fecha_inicio\", \"fecha_fin\", \"vigente\")\n",
    "\n",
    "def extract_fecha_from_multidimensional(spark):\n",
    "    df = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", db_multidimensional_connection_string)\\\n",
    "        .option(\"dbtable\", \"fecha\")\\\n",
    "        .option(\"user\", user_md)\\\n",
    "        .option(\"password\", psswd_md)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load()\n",
    "    return df.select(\"id\", \"year\", \"month\", \"day\")\n",
    "\n",
    "\n",
    "def extract_empresa_from_multidimensional(spark):\n",
    "    df = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", db_multidimensional_connection_string)\\\n",
    "        .option(\"dbtable\", \"empresa\")\\\n",
    "        .option(\"user\", user_md)\\\n",
    "        .option(\"password\", psswd_md)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "def extract_equipo_from_multidimensional(spark):\n",
    "    df = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", db_multidimensional_connection_string)\\\n",
    "        .option(\"dbtable\", \"tipo_equipo\")\\\n",
    "        .option(\"user\", user_md)\\\n",
    "        .option(\"password\", psswd_md)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "def extract_tipo_from_multidimensional(spark):\n",
    "    df = spark.read.format(\"jdbc\")\\\n",
    "        .option(\"url\", db_multidimensional_connection_string)\\\n",
    "        .option(\"dbtable\", \"tipo_vuelo_trafico\")\\\n",
    "        .option(\"user\", user_md)\\\n",
    "        .option(\"password\", psswd_md)\\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "\n",
    "def transform_aeropuertos(df_aeropuerto):\n",
    "    dim_aeropuerto = df_aeropuerto.filter(df_aeropuerto.nombre != 'CAMILO DAZA No.2')\n",
    "    modificaciones = dim_aeropuerto.groupBy('sigla').count()\n",
    "\n",
    "    dim_aeropuerto = dim_aeropuerto.join(modificaciones, how='inner', on=['sigla'])\n",
    "    dim_aeropuerto = dim_aeropuerto.withColumn('fecha_inicio',\n",
    "                                               F.when((F.col('Ano') == 2014), '2009-01-01')\n",
    "                                               .when((F.col('Ano') == 2015) & (F.col('count') == 2), '2009-01-01')\n",
    "                                               .when((F.col('Ano') == 2015) & (F.col('count') == 3), '2015-01-01')\n",
    "                                               .when((F.col('Ano') == 2016), '2016-01-01')\n",
    "                                               .otherwise(0)\n",
    "                                               )\n",
    "    dim_aeropuerto = dim_aeropuerto.withColumn('fecha_fin',\n",
    "                                               F.when((F.col('Ano') == 2014), '2014-12-31')\n",
    "                                               .when((F.col('Ano') == 2015), '2015-12-31')\n",
    "                                               .when((F.col('Ano') == 2016), '2100-12-31')\n",
    "                                               .otherwise(0)\n",
    "                                               )\n",
    "\n",
    "    dim_aeropuerto = dim_aeropuerto.withColumn('vigente',\n",
    "                                               F.when((F.col('Ano') == 2014), 'N')\n",
    "                                               .when((F.col('Ano') == 2015), 'N')\n",
    "                                               .when((F.col('Ano') == 2016), 'S')\n",
    "                                               .otherwise(0)\n",
    "                                               )\n",
    "    return dim_aeropuerto.withColumn(\"iata\", F.col(\"sigla\")).withColumnRenamed(\"Ano\", \"year\").drop(\"count\")\n",
    "\n",
    "\n",
    "def transform_vuelos(vuelos_df, aeropuertos_df, fecha_df, empresa_df, equipo_df, tipo_df):\n",
    "    df = vuelos_df.selectExpr('ano', 'mes', 'origen', 'destino', 'sillas as puestos_disponibles', 'carga_ofrecida as carga_ofrecida', 'pasajeros as pasajeros_transportado', 'carga_bordo as carga_transportado', 'vuelos', 'empresa', 'tipo_equipo', 'tipo_vuelo', 'trafico')\n",
    "\n",
    "    df = df.withColumn('date', F.concat(F.col('ano'),F.lit('-'), F.col('mes'), F.lit('-01')))\n",
    "    df = df.withColumn('date', df['date'].cast(DateType()))\n",
    "\n",
    "    df = df.withColumn(\"pasajeros_desaprovechado\", F.col(\"puestos_disponibles\") - F.col(\"pasajeros_transportado\"))\n",
    "    df = df.withColumn(\"carga_desaprovechado\", F.col(\"carga_ofrecida\") - F.col(\"carga_transportado\"))\n",
    "\n",
    "    tmp_df = fecha_df.withColumnRenamed(\"id\", \"id_fecha\")\n",
    "    df = df.join(tmp_df, (df['ano'] == tmp_df['year']) & (df['mes']==tmp_df['month']) & (1==tmp_df['day']))\n",
    "\n",
    "    tmp_df = aeropuertos_df.withColumnRenamed(\"id\", \"id_origen\").withColumnRenamed(\"sigla\", \"sigla_origen\").select(\"id_origen\", \"sigla_origen\", \"fecha_inicio\", \"fecha_fin\")\n",
    "    tmp_df = tmp_df.withColumn('fecha_inicio', tmp_df['fecha_inicio'].cast(DateType()))\n",
    "    tmp_df = tmp_df.withColumn('fecha_fin', tmp_df['fecha_fin'].cast(DateType()))\n",
    "    df = df.join(tmp_df, (df['origen'] == tmp_df['sigla_origen']) & (tmp_df['fecha_inicio'] <= df['date']) & (df['date'] <= tmp_df['fecha_fin']))\n",
    "\n",
    "\n",
    "    tmp_df = aeropuertos_df.withColumnRenamed(\"id\", \"id_destino\").withColumnRenamed(\"sigla\", \"sigla_destino\").select(\"id_destino\", \"sigla_destino\", \"fecha_inicio\", \"fecha_fin\")\n",
    "    tmp_df = tmp_df.withColumn('fecha_inicio', tmp_df['fecha_inicio'].cast(DateType()))\n",
    "    tmp_df = tmp_df.withColumn('fecha_fin', tmp_df['fecha_fin'].cast(DateType()))\n",
    "    df = df.join(tmp_df, (df['destino'] == tmp_df['sigla_destino']) & (tmp_df['fecha_inicio'] <= df['date']) & (df['date'] <= tmp_df['fecha_fin']))\n",
    "\n",
    "    tmp_df = empresa_df.withColumnRenamed(\"id\", \"id_empresa\")\n",
    "    df = df.join(tmp_df, df['empresa'] == tmp_df['empresa'])\n",
    "\n",
    "    tmp_df = equipo_df.withColumnRenamed(\"id\", \"id_equipo\")\n",
    "    df = df.join(tmp_df, df['tipo_equipo'] == tmp_df['equipo'])\n",
    "\n",
    "    tmp_df = tipo_df.withColumnRenamed(\"id\", \"id_tipo\")\n",
    "    df = df.join(tmp_df, (df['tipo_vuelo'] == tmp_df['vuelo']) & (df['trafico'] == tmp_df['trafico']))\n",
    "\n",
    "    return df.select('id_fecha', 'id_origen', 'id_destino','id_equipo', 'id_empresa', 'id_tipo', 'puestos_disponibles','carga_ofrecida','pasajeros_transportado', 'carga_transportado', 'pasajeros_desaprovechado', 'carga_desaprovechado', 'vuelos')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejecución del proceso ETL."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# ETL aeropuertos\n",
    "aeropuertos_df = extract_aeropuertos(spark)\n",
    "aeropuertos_df = transform_aeropuertos(aeropuertos_df)\n",
    "\n",
    "aeropuertos_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"aeropuertos\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "empresa_df, equipo_df, tipo_vuelotrafico_df = extract_otros(spark)\n",
    "\n",
    "empresa_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"empresa\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()\n",
    "\n",
    "equipo_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"tipo_equipo\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()\n",
    "\n",
    "\n",
    "tipo_vuelotrafico_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"tipo_vuelo_trafico\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "(82559, 13)"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ETL tabla hechos\n",
    "vuelos_df = extract_vuelos(spark)\n",
    "fecha_df = extract_fecha_from_multidimensional(spark)\n",
    "aeropuertos_df = extract_aeropuertos_from_multidimensional(spark)\n",
    "empresa_df = extract_empresa_from_multidimensional(spark)\n",
    "equipo_df = extract_equipo_from_multidimensional(spark)\n",
    "tipo_vuelotrafico_df = extract_tipo_from_multidimensional(spark)\n",
    "vuelos_df = transform_vuelos(vuelos_df, aeropuertos_df, fecha_df, empresa_df, equipo_df, tipo_vuelotrafico_df)\n",
    "\n",
    "vuelos_df.toPandas().shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "vuelos_df.select('*').write.format('jdbc')\\\n",
    "    .mode('append')\\\n",
    "    .option(\"url\", db_multidimensional_connection_string)\\\n",
    "    .option(\"dbtable\", \"hecho_vuelos\")\\\n",
    "    .option(\"user\", user_md)\\\n",
    "    .option(\"password\", psswd_md)\\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}